AQLM quantization process:

---
### **Using Your Dataset Format with AQLM**

#### **1. Dataset Format Validation**
Ensure the dataset is structured as follows:
```json
{"messages": ["a", "b"]}
{"messages": ["x", "y", "z"]}
{"messages": ["p", "q"]}
```
Where:
- Each JSON object is on a new line.
- The `messages` key contains an array of strings representing turns in a conversation.

#### **2. Preparing the Dataset for AQLM**
AQLM expects a standard text-based dataset. You can preprocess your JSONL dataset into plain text by concatenating the `messages` for each JSON object into a single string.

##### Example Preprocessing Script (Python):
```python
import json

input_file = "dataset.jsonl"
output_file = "processed_dataset.txt"

with open(input_file, "r") as infile, open(output_file, "w") as outfile:
    for line in infile:
        data = json.loads(line)
        messages = data.get("messages", [])
        # Concatenate messages with spaces or newlines (choose based on model training)
        conversation = " ".join(messages)
        outfile.write(conversation + "\n")

print(f"Processed dataset saved to {output_file}")
```

This script will create a `processed_dataset.txt` file where each conversation is represented as a single line.

##### Example Output:
```plaintext
a b
x y z
p q
```

#### **3. Using the Preprocessed Dataset**
Set `DATASET_PATH` to point to the processed dataset:
```bash
export DATASET_PATH=/path/to/processed_dataset.txt
```


To quantize your fine-tuned model `research_outputs_merged` using the AQLM approach and save it to the directory `research_outputs_AQLM`, follow these steps:

---

### **Steps for AQLM Quantization**

1. **Set Up Environment Variables**
   Replace the placeholders with your actual paths and settings:
   ```bash
   export CUDA_VISIBLE_DEVICES=0  # Use appropriate GPUs
   export MODEL_PATH=/path/to/research_outputs_merged
   export DATASET_PATH=/path/to/processed_dataset.txt  # Preprocessed dataset as plain text
   export SAVE_PATH=/path/to/research_outputs_AQLM
   export WANDB_PROJECT=AQLM_QUANTIZATION
   export WANDB_NAME=Mistral7B_AQLM
   ```

2. **Run the AQLM Script**
   Execute the quantization script with arguments tailored for your requirements:
   ```bash
   python main.py $MODEL_PATH $DATASET_PATH \
     --nsamples=1024 \
     --val_size=128 \
     --num_codebooks=1 \
     --nbits_per_codebook=4 \
     --in_group_size=8 \
     --relative_mse_tolerance=0.01 \
     --finetune_batch_size=32 \
     --finetune_max_epochs=10 \
     --finetune_early_stop=3 \
     --finetune_keep_best \
     --local_batch_size=1 \
     --offload_activations \
     --wandb \
     --resume \
     --save $SAVE_PATH
   ```

---

### **Details of Key Parameters**
| **Parameter**                 | **Value**                                  | **Explanation**                                                                 |
|-------------------------------|--------------------------------------------|---------------------------------------------------------------------------------|
| `MODEL_PATH`                  | `/path/to/research_outputs_merged`         | Path to your fine-tuned model checkpoint.                                       |
| `DATASET_PATH`                | `/path/to/processed_dataset.txt`           | Preprocessed dataset in plain text format.                                      |
| `SAVE_PATH`                   | `/path/to/research_outputs_AQLM`           | Directory where the quantized model will be saved.                              |
| `--nbits_per_codebook`        | `4`                                        | Specifies 4-bit quantization.                                                   |
| `--nsamples`                  | `1024`                                     | Number of samples to use for quantization calibration.                          |
| `--val_size`                  | `128`                                      | Validation set size for fine-tuning.                                            |
| `--finetune_max_epochs`       | `10`                                       | Maximum number of fine-tuning epochs.                                           |
| `--finetune_early_stop`       | `3`                                        | Stops fine-tuning early if no improvement is seen after 3 epochs.               |
| `--offload_activations`       | Enabled                                    | Reduces GPU memory usage by offloading activations to CPU during fine-tuning.   |
| `--wandb`                     | Enabled                                    | Logs metrics and progress to Weights & Biases for better experiment tracking.   |

---

### **Final Output**
- The 4-bit quantized model will be saved in the directory:
  ```
  /path/to/research_outputs_AQLM
  ```


python ./AQLM/main.py $MODEL_PATH $DATASET_PATH \
  --nsamples 1024 \
  --val_size 128 \
  --num_codebooks 1 \
  --nbits_per_codebook 4 \
  --in_group_size 8 \
  --relative_mse_tolerance 0.01 \
  --finetune_batch_size 32 \
  --finetune_max_epochs 10 \
  --finetune_early_stop 3 \
  --finetune_keep_best \
  --local_batch_size 1 \
  --offload_activations \
  --resume \
  --save $SAVE_PATH

- If Weights & Biases is enabled, logs and metrics will be available under the project `AQLM_QUANTIZATION` with the name `Mistral7B_AQLM`.


import torch
from transformers import AutoTokenizer

# Load the tokenizer
model_id = "mistralai/Mistral-7B-Instruct-v0.3"
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Example samples
samples = [
    {
        "system": "You are a helpful assistant.",
        "user": "What is the weather like in Paris?",
        "response": "The weather in Paris is sunny and warm."
    },
    {
        "system": "You are a knowledgeable assistant.",
        "user": "Explain quantum computing in simple terms.",
        "response": "Quantum computing is a type of computing that uses qubits to process information in ways classical computers cannot."
    }
]

# Function to format and tokenize the responses
def format_and_tokenize_responses(samples, tokenizer, max_length=2048):
    tokenized_responses = []
    for sample in samples:
        # Format the prompt in Mistral v3 format
        formatted_prompt = (
            f"<s>[INST] {sample['system']}\n\n{sample['user']} [/INST] {sample['response']}</s>"
        )
        # Tokenize the formatted prompt
        tokenized = tokenizer(
            formatted_prompt,
            max_length=max_length,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )
        # Append only the tokenized response
        tokenized_responses.append(tokenized["input_ids"])
    
    # Stack all tokenized responses into a tensor
    return torch.stack(tokenized_responses)

# Tokenize the responses
tokenized_responses = format_and_tokenize_responses(samples, tokenizer)

# Save the tokenized responses to a .pt file
output_file = "tokenized_responses.pt"
torch.save(tokenized_responses, output_file)
print(f"Tokenized responses saved to {output_file}")


python convert_to_hf.py \
    mistralai/Mistral-7B-Instruct-v0.3 \
    /path/to/aqlm_quantized_checkpoint \
    /path/to/huggingface_compatible_model \
    --save_safetensors \
    --trust_remote_code \
    --load_model \
    --save_tokenizer
